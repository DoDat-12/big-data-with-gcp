{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc73781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dfa7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 14:41:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Start Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Trip per pickup location\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d710d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path lists\n",
    "zone_lookup = \"hdfs://10.128.0.59:8020/raw_data/taxi_zone_lookup.csv\"\n",
    "fact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\n",
    "dim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\n",
    "dim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\n",
    "dim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\n",
    "dim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\n",
    "dim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\n",
    "dim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n",
    "\n",
    "# uber-analysis-439804.query_result. + the table's name\n",
    "output = \"uber-analysis-439804.query_result.trips_per_pu_location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2c15fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data into dataframe\n",
    "df_fact = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", fact_trip) \\\n",
    "    .load() \\\n",
    "    .select(\"trip_id\", \"datetimestamp_id\", \"pu_location_id\")\n",
    "\n",
    "df_datetime = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", dim_datetime) \\\n",
    "    .load() \\\n",
    "    .select(\"pick_year\", \"datetime_id\")\n",
    "\n",
    "df_pickup_location = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", dim_pickup_location) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6156ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining\n",
    "df_joined = df_fact.alias(\"fact_data\") \\\n",
    "    .join(df_datetime.alias(\"dim_datetime\"),\n",
    "          col(\"fact_data.datetimestamp_id\") == col(\"dim_datetime.datetime_id\"), \"inner\") \\\n",
    "    .join(broadcast(df_pickup_location.alias(\"dim_pickup_location\")),\n",
    "          col(\"fact_data.pu_location_id\") == col(\"dim_pickup_location.PULocationID\"), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"fact_data.trip_id\").alias(\"trip_id\"),\n",
    "        col(\"dim_datetime.pick_year\").alias(\"year\"),\n",
    "        col(\"dim_pickup_location.PULocationID\").alias(\"LocationID\"),\n",
    "        col(\"dim_pickup_location.Borough\").alias(\"Borough\"),\n",
    "        col(\"dim_pickup_location.Zone\").alias(\"Zone\"),\n",
    "        col(\"dim_pickup_location.service_zone\").alias(\"service_zone\")\n",
    ")\n",
    "\n",
    "# Aggregation\n",
    "df_result = df_joined.groupBy(\"year\", \"LocationID\", \"Borough\", \"Zone\", \"service_zone\") \\\n",
    "    .agg(count(\"trip_id\").alias(\"total_trips\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2c54b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save to BigQuery\n",
    "df_result.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", output) \\\n",
    "    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051e06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
