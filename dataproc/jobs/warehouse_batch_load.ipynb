{"cells": [{"cell_type": "markdown", "id": "b9997af5", "metadata": {}, "source": "# Batch Load to Data Warehouse\n\n### From 2015 to 2024"}, {"cell_type": "code", "execution_count": 1, "id": "f28c8f2f", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 2, "id": "ab4e17ca", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/17 08:28:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Start Spark Session\nbatch_year = 2024\n\nspark = SparkSession \\\n    .builder \\\n    .appName(f\"Batch $batch_year\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "3765328e", "metadata": {}, "outputs": [], "source": "# Path lists\nzone_lookup = \"hdfs://10.128.0.59:8020/raw_data/taxi_zone_lookup.csv\"\ninput_path = \"hdfs://10.128.0.59:8020/raw_data/{}\".format(batch_year)\n\noutput_fact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\noutput_dim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\noutput_dim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\noutput_dim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\noutput_dim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\noutput_dim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\noutput_dim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\""}, {"cell_type": "markdown", "id": "7b39237f", "metadata": {}, "source": "### Input"}, {"cell_type": "code", "execution_count": 4, "id": "bc34468e", "metadata": {}, "outputs": [], "source": "# Schema\ninput_schema = StructType([\n    StructField(\"VendorID\", LongType(), True),\n    StructField(\"tpep_pickup_datetime\", TimestampNTZType(), True),\n    StructField(\"tpep_dropoff_datetime\", TimestampNTZType(), True),\n    StructField(\"passenger_count\", LongType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"RatecodeID\", LongType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"PULocationID\", LongType(), True),\n    StructField(\"DOLocationID\", LongType(), True),\n    StructField(\"payment_type\", LongType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])\n\nlookup_schema = StructType([\n    StructField(\"LocationID\", IntegerType(), True),\n    StructField(\"Borough\", StringType(), True),\n    StructField(\"Zone\", StringType(), True),\n    StructField(\"service_zone\", StringType(), True),\n])"}, {"cell_type": "code", "execution_count": 5, "id": "addb4e54", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Get last ID\ndf_prev_fact = spark.read.format(\"parquet\") \\\n    .option(\"path\", output_fact_trip) \\\n    .load() \\\n    .select(\"trip_id\")\nmax_fact_id = df_prev_fact.agg({\"trip_id\": \"max\"}).collect()[0][0]\n\ndf_prev_datetime = spark.read.format(\"parquet\") \\\n    .option(\"path\", output_dim_datetime) \\\n    .load() \\\n    .select(\"datetime_id\")\nmax_datetime_id = df_prev_datetime.agg({\"datetime_id\": \"max\"}).collect()[0][0]"}, {"cell_type": "code", "execution_count": 6, "id": "e1292a33", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n |-- passenger_count: long (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: long (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- trip_id: long (nullable = false)\n\nroot\n |-- LocationID: integer (nullable = true)\n |-- Borough: string (nullable = true)\n |-- Zone: string (nullable = true)\n |-- service_zone: string (nullable = true)\n\n"}], "source": "df_input = spark.read.format(\"parquet\") \\\n    .schema(input_schema) \\\n    .load(input_path) \\\n    .dropna() \\\n    .filter((year(col(\"tpep_pickup_datetime\")) == batch_year) & \\\n            (col(\"trip_distance\") > 0.0) & \\\n            (col(\"passenger_count\") > 0)) \\\n    .withColumn(\"trip_id\", monotonically_increasing_id() + max_fact_id + 1)\ndf_input.printSchema()\n\ndf_lookup = spark.read.format(\"csv\") \\\n    .schema(lookup_schema) \\\n    .option(\"header\", True) \\\n    .load(zone_lookup) \\\n    .dropna()\ndf_lookup.printSchema()"}, {"cell_type": "code", "execution_count": 7, "id": "12ea6d32", "metadata": {}, "outputs": [], "source": "# SCD Type I\ndim_vendor = spark.read.format(\"parquet\") \\\n    .load(output_dim_vendor)\n\ndim_rate_code = spark.read.format(\"parquet\") \\\n    .load(output_dim_rate_code)\n\ndim_payment = spark.read.format(\"parquet\") \\\n    .load(output_dim_payment)"}, {"cell_type": "markdown", "id": "8ba99504", "metadata": {}, "source": "### Output"}, {"cell_type": "code", "execution_count": 8, "id": "7b1d7406", "metadata": {}, "outputs": [], "source": "# Datetime dimension\ndim_datetime = df_input \\\n    .select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\") \\\n    .distinct() \\\n    .withColumn(\"datetime_id\", monotonically_increasing_id() + max_datetime_id + 1) \\\n    .withColumn(\"pick_hour\", hour(col(\"tpep_pickup_datetime\")) + minute(col(\"tpep_pickup_datetime\")) / 60.0) \\\n    .withColumn(\"pick_day\", dayofmonth(col(\"tpep_pickup_datetime\"))) \\\n    .withColumn(\"pick_month\", month(col(\"tpep_pickup_datetime\"))) \\\n    .withColumn(\"pick_year\", year(col(\"tpep_pickup_datetime\"))) \\\n    .withColumn(\"pick_weekday\", F.date_format(col(\"tpep_pickup_datetime\"), \"EEEE\")) \\\n    .withColumn(\"drop_hour\", hour(col(\"tpep_dropoff_datetime\")) + minute(col(\"tpep_dropoff_datetime\")) / 60.0) \\\n    .withColumn(\"drop_day\", dayofmonth(col(\"tpep_dropoff_datetime\"))) \\\n    .withColumn(\"drop_month\", month(col(\"tpep_dropoff_datetime\"))) \\\n    .withColumn(\"drop_year\", year(col(\"tpep_dropoff_datetime\"))) \\\n    .withColumn(\"drop_weekday\", F.date_format(col(\"tpep_pickup_datetime\"), \"EEEE\"))"}, {"cell_type": "code", "execution_count": 9, "id": "5cd49bdd", "metadata": {}, "outputs": [], "source": "# Pickup location dimension\n# PULocationID + Borough + Zone + service_zone\ndim_pickup_location = df_input \\\n    .select(\"PULocationID\") \\\n    .distinct() \\\n    .join(df_lookup, df_input.PULocationID == df_lookup.LocationID, \"inner\") \\\n    .select(\"PULocationID\", \"Borough\", \"Zone\", \"service_zone\")\n\n# Dropoff location dimension\n# DOLocationID + Borough + Zone + service_zone\ndim_dropoff_location = df_input \\\n    .select(\"DOLocationID\") \\\n    .distinct() \\\n    .join(df_lookup, df_input.DOLocationID == df_lookup.LocationID, \"inner\") \\\n    .select(\"DOLocationID\", \"Borough\", \"Zone\", \"service_zone\")"}, {"cell_type": "code", "execution_count": 10, "id": "9a26015c", "metadata": {}, "outputs": [], "source": "# Fact table\nfact_trip = df_input.alias(\"fact_data\") \\\n    .join(dim_datetime.alias(\"dim_datetime\"), (col(\"fact_data.tpep_pickup_datetime\") == col(\"dim_datetime.tpep_pickup_datetime\")) & (col(\"fact_data.tpep_dropoff_datetime\") == col(\"dim_datetime.tpep_dropoff_datetime\")), \"inner\") \\\n    .join(dim_pickup_location.alias(\"dim_pickup_location\"), col(\"fact_data.PULocationID\") == col(\"dim_pickup_location.PULocationID\"), \"inner\") \\\n    .join(dim_dropoff_location.alias(\"dim_dropoff_location\"), col(\"fact_data.DOLocationID\") == col(\"dim_dropoff_location.DOLocationID\"), \"inner\") \\\n    .join(broadcast(dim_vendor.alias(\"dim_vendor\")), col(\"fact_data.VendorID\") == col(\"dim_vendor.VendorID\"), \"inner\") \\\n    .join(broadcast(dim_rate_code.alias(\"dim_ratecode\")), col(\"fact_data.RatecodeID\") == col(\"dim_ratecode.RatecodeID\"), \"inner\") \\\n    .join(broadcast(dim_payment.alias(\"dim_payment\")), col(\"fact_data.payment_type\") == col(\"dim_payment.payment_type\"), \"inner\") \\\n    .select(\n        col(\"fact_data.trip_id\"),\n        col(\"dim_vendor.VendorID\").alias(\"vendor_id\"),\n        col(\"dim_datetime.datetime_id\").alias(\"datetimestamp_id\"),\n        col(\"dim_pickup_location.PULocationID\").alias(\"pu_location_id\"),\n        col(\"dim_dropoff_location.DOLocationID\").alias(\"do_location_id\"),\n        col(\"dim_ratecode.RatecodeID\").alias(\"ratecode_id\"),\n        col(\"dim_payment.payment_type\").alias(\"payment_id\"),\n        col(\"fact_data.passenger_count\"),\n        col(\"fact_data.trip_distance\"),\n        col(\"fact_data.fare_amount\"),\n        col(\"fact_data.extra\"),\n        col(\"fact_data.mta_tax\"),\n        col(\"fact_data.tip_amount\"),\n        col(\"fact_data.tolls_amount\"),\n        col(\"fact_data.total_amount\")\n        # delete cause of version conflict\n        # df.congestion_surcharge,\n        # df.Airport_fee,\n        # df.improvement_surcharge,\n)"}, {"cell_type": "markdown", "id": "9fdba267", "metadata": {}, "source": "### Append to HDFS"}, {"cell_type": "code", "execution_count": 11, "id": "0f03ee6e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# fact_trip_partitioned = fact_trip.coalesce(6)\n\nfact_trip.write \\\n    .format(\"parquet\") \\\n    .option(\"path\", output_fact_trip) \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 12, "id": "ed0b4ea4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# dim_datetime_partitioned = dim_datetime.coalesce(6)\n\ndim_datetime.write \\\n    .partitionBy(\"pick_year\", \"pick_month\") \\\n    .format(\"parquet\") \\\n    .option(\"path\", output_dim_datetime) \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 13, "id": "ffec44f1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "dim_prev_pu = spark.read.format(\"parquet\") \\\n    .load(output_dim_pickup_location)\ndim_append_pu = dim_pickup_location.subtract(dim_prev_pu)\ndim_append_pu.write \\\n    .partitionBy(\"service_zone\") \\\n    .format(\"parquet\") \\\n    .option(\"path\", output_dim_pickup_location) \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 14, "id": "a17a2ec8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "dim_prev_do = spark.read.format(\"parquet\") \\\n    .load(output_dim_dropoff_location)\ndim_append_do = dim_dropoff_location.subtract(dim_prev_do)\ndim_append_do.write \\\n    .partitionBy(\"service_zone\") \\\n    .format(\"parquet\") \\\n    .option(\"path\", output_dim_dropoff_location) \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 15, "id": "a0e70788", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "5e24f57d", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}