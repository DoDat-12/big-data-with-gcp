{"cells": [{"cell_type": "markdown", "id": "98441f01", "metadata": {}, "source": "## Number of trips per day each year"}, {"cell_type": "code", "execution_count": 1, "id": "8dc73781", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 2, "id": "67dfa7f2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/25 16:10:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Start Spark Session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Example\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "d710d31c", "metadata": {}, "outputs": [], "source": "# Path lists\nzone_lookup = \"hdfs://10.128.0.59:8020/raw_data/taxi_zone_lookup.csv\"\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\ndim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\ndim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\ndim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\ndim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n\n# uber-analysis-439804.query_result. + the table's name\noutput = \"uber-analysis-439804.query_result.trips_per_day\""}, {"cell_type": "code", "execution_count": 4, "id": "7f2c15fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read data into dataframe\ndf_fact = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", fact_trip) \\\n    .load() \\\n    .select(\"trip_id\", \"datetimestamp_id\")\n\ndf_datetime = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_datetime) \\\n    .load() \\\n    .select(\"pick_year\", \"pick_weekday\", \"datetime_id\")"}, {"cell_type": "code", "execution_count": 5, "id": "1df2ed61", "metadata": {}, "outputs": [], "source": "# SELECT year, day, COUNT(trip_id)\n# FROM df_fact \n#     INNER JOIN df_datetime \n#         ON df_fact.datetimestamp_id = df_datetime.datetime_id\n# GROUP BY year, day;"}, {"cell_type": "code", "execution_count": 6, "id": "b6156ed8", "metadata": {}, "outputs": [], "source": "# Join\ndf_joined = df_fact.join(df_datetime, \n                         df_fact.datetimestamp_id == df_datetime.datetime_id,\n                        \"inner\")\n\n# Query\ndf_result = df_joined.groupBy(\"pick_year\", \"pick_weekday\") \\\n    .agg(count(\"trip_id\").alias(\"total_trips\")) \\\n    .select(\n        col(\"pick_year\").alias(\"year\"),\n        col(\"pick_weekday\").alias(\"day\"),\n        col(\"total_trips\")\n    )\n\n# df_result.show()"}, {"cell_type": "code", "execution_count": 7, "id": "bc2c54b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Save to BigQuery\ndf_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 8, "id": "051e06ba", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}