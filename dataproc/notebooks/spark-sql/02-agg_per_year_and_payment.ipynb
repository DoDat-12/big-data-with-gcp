{"cells": [{"cell_type": "markdown", "id": "263e2b4a", "metadata": {}, "source": "## Aggregation Each Year and Aggregation Payment"}, {"cell_type": "code", "execution_count": 1, "id": "8dc73781", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 2, "id": "67dfa7f2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/28 16:17:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Start Spark Session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"General Aggregation Per Year\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "d710d31c", "metadata": {}, "outputs": [], "source": "# Path lists\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\ndim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n\n# uber-analysis-439804.query_result. + the table's name\noutput_year = \"uber-analysis-439804.query_result.agg_per_year\"\noutput_payment_year = \"uber-analysis-439804.query_result.payment_per_year\"\noutput_payment = \"uber-analysis-439804.query_result.agg_per_payment\""}, {"cell_type": "code", "execution_count": 4, "id": "7f2c15fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read data into dataframe\ndf_fact = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", fact_trip) \\\n    .load()\n\ndf_datetime = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_datetime) \\\n    .load() \\\n    .select(\"pick_year\", \"datetime_id\")\n\ndf_payment = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_payment) \\\n    .load() \\\n    .filter(~col(\"payment_type\").isin([4, 6]))"}, {"cell_type": "code", "execution_count": 5, "id": "b6156ed8", "metadata": {}, "outputs": [], "source": "# Join\ndf_joined = df_fact \\\n    .join(df_datetime, \n          df_fact.datetimestamp_id == df_datetime.datetime_id, \"inner\") \\\n    .join(broadcast(df_payment),\n          df_fact.payment_id == df_payment.payment_type, \"inner\")\n\n# Aggregation each year\ndf_year_result = df_joined.groupBy(\"pick_year\") \\\n    .agg(\n        count(\"trip_id\").alias(\"total_trips\"),\n        sum(\"passenger_count\").alias(\"total_passengers\"),\n        sum(\"trip_distance\").alias(\"total_distance\"),\n        sum(\"total_amount\").alias(\"total_amount\"),\n        avg(\"passenger_count\").alias(\"average_passengers_per_trip\"),\n        avg(\"trip_distance\").alias(\"average_distance_per_trip\"),\n        avg(\"total_amount\").alias(\"average_amount_per_trip\")) \\\n    .select(\n        col(\"pick_year\").alias(\"year\"),\n        col(\"total_trips\"),\n        col(\"total_passengers\"),\n        col(\"total_distance\"),\n        col(\"total_amount\"),\n        col(\"average_passengers_per_trip\"),\n        col(\"average_distance_per_trip\"),\n        col(\"average_amount_per_trip\")\n    )\n\ndf_payment_each_year_result = df_joined.groupBy(\"pick_year\", \"payment_id\", \"payment_type_name\") \\\n    .agg(count(\"trip_id\").alias(\"total_trips\")) \\\n    .select(\n        col(\"pick_year\").alias(\"year\"),\n        col(\"payment_id\"),\n        col(\"payment_type_name\").alias(\"payment_name\"),\n        col(\"total_trips\")\n    )\n\ndf_payment_result = df_joined.groupBy(\"payment_id\", \"payment_type_name\") \\\n    .agg(count(\"trip_id\").alias(\"total_trips\")) \\\n    .select(\n        col(\"payment_id\"),\n        col(\"payment_type_name\").alias(\"payment_name\"),\n        col(\"total_trips\")\n    )"}, {"cell_type": "code", "execution_count": 6, "id": "bc2c54b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/28 16:17:50 WARN DAGScheduler: Broadcasting large task binary with size 1049.8 KiB\n24/12/28 16:26:27 WARN DAGScheduler: Broadcasting large task binary with size 1159.5 KiB\n24/12/28 16:31:37 WARN DAGScheduler: Broadcasting large task binary with size 1387.6 KiB\n24/12/28 16:31:46 WARN DAGScheduler: Broadcasting large task binary with size 1047.7 KiB\n24/12/28 16:38:04 WARN DAGScheduler: Broadcasting large task binary with size 1122.4 KiB\n24/12/28 16:43:59 WARN DAGScheduler: Broadcasting large task binary with size 1343.9 KiB\n24/12/28 16:44:06 WARN DAGScheduler: Broadcasting large task binary with size 1047.7 KiB\n24/12/28 16:59:16 WARN DAGScheduler: Broadcasting large task binary with size 1121.3 KiB\n24/12/28 17:11:14 WARN DAGScheduler: Broadcasting large task binary with size 1342.6 KiB\n                                                                                \r"}], "source": "# Save to BigQuery\ndf_year_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output_year) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()\n\ndf_payment_each_year_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output_payment_year) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()\n\ndf_payment_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output_payment) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 7, "id": "051e06ba", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "acd06af2", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}