{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8dc73781", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 2, "id": "67dfa7f2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/25 14:38:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Start Spark Session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Aggregation Payment Per Year\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "d710d31c", "metadata": {}, "outputs": [], "source": "# Path lists\nzone_lookup = \"hdfs://10.128.0.59:8020/raw_data/updated_zone_lookup.csv\"\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\ndim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\ndim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\ndim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\ndim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n\n# uber-analysis-439804.query_result. + the table's name\noutput = \"uber-analysis-439804.query_result.payment_per_year\""}, {"cell_type": "code", "execution_count": 4, "id": "7f2c15fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read data into dataframe\ndf_fact = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", fact_trip) \\\n    .load() \\\n    .select(\"trip_id\", \"datetimestamp_id\", \"payment_id\")\n\ndf_datetime = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_datetime) \\\n    .load() \\\n    .select(\"datetime_id\", \"pick_year\")\n\ndf_payment = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_payment) \\\n    .load() \\\n    .filter(~col(\"payment_type\").isin([4, 6]))\n\n# df_payment.show()"}, {"cell_type": "code", "execution_count": 5, "id": "1df2ed61", "metadata": {}, "outputs": [], "source": "# SELECT\n#     d.pick_year AS year,\n#     p.payment_type AS payment_id,\n#     p.payment_type_name AS payment_name,\n#     COUNT(f.trip_id) AS total_trips\n# FROM df_fact f\n#     INNER JOIN df_datetime d\n#         ON f.datetimestamp_id = d.datetime_id\n#     INNER JOIN df_payment p\n#         ON f.payment_id = p.payment_type\n# WHERE p.payment_type NOT IN (4, 6);"}, {"cell_type": "code", "execution_count": 6, "id": "b6156ed8", "metadata": {}, "outputs": [], "source": "# Join\ndf_joined = df_fact.alias(\"fact_data\") \\\n    .join(df_datetime.alias(\"dim_datetime\"),\n         col(\"fact_data.datetimestamp_id\") == col(\"dim_datetime.datetime_id\"), \"inner\") \\\n    .join(broadcast(df_payment.alias(\"dim_payment\")),\n         col(\"fact_data.payment_id\") == col(\"dim_payment.payment_type\"), \"inner\") \\\n    .select(\n        col(\"fact_data.trip_id\").alias(\"trip_id\"),\n        col(\"dim_datetime.pick_year\").alias(\"year\"),\n        col(\"dim_payment.payment_type\").alias(\"payment_id\"),\n        col(\"dim_payment.payment_type_name\").alias(\"payment_name\")\n    )\n\n# df_joined.show(5, truncate = False)\n\n# Query\ndf_result = df_joined.groupBy(\"year\", \"payment_id\", \"payment_name\") \\\n    .agg(count(\"trip_id\").alias(\"total_trips\")) \\\n    .select(\n        col(\"year\"),\n        col(\"payment_id\"),\n        col(\"payment_name\"),\n        col(\"total_trips\")\n    )\n\n# df_result.show()"}, {"cell_type": "code", "execution_count": 7, "id": "bc2c54b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/25 14:39:12 WARN DAGScheduler: Broadcasting large task binary with size 1047.7 KiB\n24/12/25 14:47:53 WARN DAGScheduler: Broadcasting large task binary with size 1129.6 KiB\n24/12/25 15:05:21 WARN DAGScheduler: Broadcasting large task binary with size 1351.0 KiB\n                                                                                \r"}], "source": "# Save to BigQuery\ndf_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 8, "id": "051e06ba", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "d34110fb", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}