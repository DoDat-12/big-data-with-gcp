{"cells": [{"cell_type": "markdown", "id": "f3069aef", "metadata": {}, "source": "## Number of trips per dropoff location"}, {"cell_type": "code", "execution_count": 1, "id": "8dc73781", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 2, "id": "67dfa7f2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/12/25 15:44:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Start Spark Session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Trip per dropoff location\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "d710d31c", "metadata": {}, "outputs": [], "source": "# Path lists\nzone_lookup = \"hdfs://10.128.0.59:8020/raw_data/updated_zone_lookup.csv\"\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\ndim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\ndim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\ndim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\ndim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n\n# uber-analysis-439804.query_result. + the table's name\noutput = \"uber-analysis-439804.query_result.trips_per_do_location\""}, {"cell_type": "code", "execution_count": 4, "id": "7f2c15fc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read data into dataframe\ndf_fact = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", fact_trip) \\\n    .load() \\\n    .select(\"trip_id\", \"datetimestamp_id\", \"do_location_id\")\n\ndf_datetime = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_datetime) \\\n    .load() \\\n    .select(\"pick_year\", \"datetime_id\")\n\ndf_dropoff_location = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_dropoff_location) \\\n    .load()"}, {"cell_type": "code", "execution_count": 5, "id": "b6156ed8", "metadata": {}, "outputs": [], "source": "# Joining\ndf_joined = df_fact.alias(\"fact_data\") \\\n    .join(df_datetime.alias(\"dim_datetime\"), \n          col(\"fact_data.datetimestamp_id\") == col(\"dim_datetime.datetime_id\"), \"inner\") \\\n    .join(broadcast(df_dropoff_location.alias(\"dim_dropoff_location\")), \n          col(\"fact_data.do_location_id\") == col(\"dim_dropoff_location.DOLocationID\"), \"inner\") \\\n    .select(\n        col(\"fact_data.trip_id\").alias(\"trip_id\"),\n        col(\"dim_datetime.pick_year\").alias(\"year\"),\n        col(\"dim_dropoff_location.DOLocationID\").alias(\"LocationID\"),\n        col(\"dim_dropoff_location.X\").alias(\"dropoff_x\"),\n        col(\"dim_dropoff_location.Y\").alias(\"dropoff_y\"),\n        col(\"dim_dropoff_location.zone\").alias(\"zone\"),\n        col(\"dim_dropoff_location.borough\").alias(\"borough\"),\n        col(\"dim_dropoff_location.service_zone\").alias(\"service_zone\")\n    )\n\n# Aggregation\ndf_result = df_joined \\\n    .groupBy(\"year\", \"LocationID\", \"dropoff_x\", \"dropoff_y\", \"zone\", \"borough\", \"service_zone\") \\\n    .agg(count(\"trip_id\").alias(\"total_trips\"))"}, {"cell_type": "code", "execution_count": 6, "id": "bc2c54b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Save to BigQuery\ndf_result.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 7, "id": "051e06ba", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}