{"cells": [{"cell_type": "markdown", "id": "23fa9dde", "metadata": {}, "source": "# Random Forest: Predicting Total Amount"}, {"cell_type": "code", "execution_count": 1, "id": "7111abc2", "metadata": {}, "outputs": [], "source": "# Import Libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import RandomForestRegressor"}, {"cell_type": "code", "execution_count": 2, "id": "a6770780", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/01 01:33:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Create SparkSession\nspark = SparkSession.builder \\\n    .appName(\"TLC Random Forest\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "00bf7846", "metadata": {}, "outputs": [], "source": "# Path lists\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\n\noutput = \"uber-analysis-439804.query_result.model_evaluation\""}, {"cell_type": "code", "execution_count": 4, "id": "a7f3874c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- trip_id: long (nullable = true)\n |-- vendor_id: long (nullable = true)\n |-- pu_location_id: long (nullable = true)\n |-- do_location_id: long (nullable = true)\n |-- ratecode_id: long (nullable = true)\n |-- payment_id: long (nullable = true)\n |-- passenger_count: long (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- pick_hour: double (nullable = true)\n |-- pick_weekday_id: integer (nullable = true)\n |-- drop_hour: double (nullable = true)\n |-- drop_weekday_id: integer (nullable = true)\n\n"}], "source": "df_fact = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", fact_trip) \\\n    .load()\n\ndf_datetime = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_datetime) \\\n    .load() \\\n    .filter(col(\"pick_year\") == 2024) \\\n    .select(\n        col(\"datetime_id\"),\n        col(\"pick_hour\"),\n        col(\"pick_weekday_id\"),\n        col(\"drop_hour\"),\n        col(\"drop_weekday_id\")\n    )\n\ndf_joined = df_fact \\\n    .join(df_datetime,\n          df_fact.datetimestamp_id == df_datetime.datetime_id, \"inner\") \\\n    .drop(col(\"datetimestamp_id\"), col(\"datetime_id\"))\n\ndf_joined.printSchema()"}, {"cell_type": "code", "execution_count": 5, "id": "0577e772", "metadata": {}, "outputs": [], "source": "selected_columns = [\n    \"vendor_id\",\n    \"pu_location_id\",\n    \"do_location_id\",\n    \"ratecode_id\",\n    \"payment_id\",\n    \"passenger_count\",\n    \"trip_distance\",\n    \"fare_amount\",\n    \"extra\",\n    \"mta_tax\",\n    \"tip_amount\",\n    \"tolls_amount\",\n    \"pick_hour\",\n    \"pick_weekday_id\",\n    \"drop_hour\",\n    \"drop_weekday_id\"\n]"}, {"cell_type": "code", "execution_count": 6, "id": "6e106d98", "metadata": {}, "outputs": [], "source": "assembler = VectorAssembler(\n    inputCols=selected_columns,\n    outputCol=\"features\"\n)\n\ndata_transformed = assembler.transform(df_joined)"}, {"cell_type": "code", "execution_count": 7, "id": "8b4c5fcb", "metadata": {}, "outputs": [], "source": "# Split dataset\ntrain_data, test_data = data_transformed.randomSplit([0.8, 0.2])"}, {"cell_type": "code", "execution_count": 8, "id": "19750377", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/01/01 02:57:49 WARN DAGScheduler: Broadcasting large task binary with size 1159.3 KiB\nWARNING: An illegal reflective access operation has occurred                    \nWARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.1.jar) to field java.nio.charset.Charset.name\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n"}], "source": "# Initialize Random Forest Regressor\nrf_model = RandomForestRegressor(\n    featuresCol=\"features\",\n    labelCol=\"total_amount\",\n    numTrees=50,        # Number of trees\n    maxDepth=7,         # Maximum depth of trees\n    seed=42             # For reproducibility\n)\n\n# Train the model\ntrained_model = rf_model.fit(train_data)\n\n# Generate predictions\npredictions = trained_model.transform(test_data)"}, {"cell_type": "code", "execution_count": null, "id": "8bd1738a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 46:========================================>            (758 + 6) / 1000]\r"}], "source": "# Evaluation\nrmse_evaluator = RegressionEvaluator(\n    labelCol=\"total_amount\",\n    predictionCol=\"prediction\",\n    metricName=\"rmse\"\n)\n\nmae_evaluator = RegressionEvaluator(\n    labelCol=\"total_amount\",\n    predictionCol=\"prediction\",\n    metricName=\"mae\"\n)\n\nr2_evaluator = RegressionEvaluator(\n    labelCol=\"total_amount\",\n    predictionCol=\"prediction\",\n    metricName=\"r2\"\n)\n\n# Calculate evaluation metrics\nrmse = rmse_evaluator.evaluate(predictions)\nmae = mae_evaluator.evaluate(predictions)\nr2 = r2_evaluator.evaluate(predictions)"}, {"cell_type": "code", "execution_count": null, "id": "19804e3f", "metadata": {}, "outputs": [], "source": "# Store in BigQuery\nevaluation_data = spark.createDataFrame([\n    Row(name=\"Random Forest\", rmse=rmse, mae=mae, r2=r2)\n])\n\nevaluation_data.show()\n\nevaluation_data.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", output) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": null, "id": "c99bb6db", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "ef877f96", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}