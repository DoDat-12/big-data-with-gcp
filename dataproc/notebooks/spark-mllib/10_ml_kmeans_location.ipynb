{"cells": [{"cell_type": "markdown", "id": "3f1cc59b", "metadata": {}, "source": "# K-Means Clustering with Pickup Location"}, {"cell_type": "code", "execution_count": 1, "id": "478dfd1d", "metadata": {}, "outputs": [], "source": "# Import\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\nfrom pyspark.ml.clustering import KMeans"}, {"cell_type": "code", "execution_count": 2, "id": "d4945f02", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Spark Version: 3.5.1\n"}, {"name": "stderr", "output_type": "stream", "text": "24/12/27 01:55:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession \\\n    .builder \\\n    .appName('Pickup Location Clustering') \\\n    .getOrCreate()\n\nprint('Spark Version: {}'.format(spark.version))"}, {"cell_type": "code", "execution_count": 3, "id": "b8e39752", "metadata": {}, "outputs": [], "source": "# Path lists\nzone_lookup = \"hdfs://10.128.0.59:8020/raw_data/updated_zone_lookup.csv\"\nfact_trip = \"hdfs://10.128.0.59:8020/data_warehouse/fact_trip\"\ndim_vendor = \"hdfs://10.128.0.59:8020/data_warehouse/dim_vendor\"\ndim_datetime = \"hdfs://10.128.0.59:8020/data_warehouse/dim_datetime\"\ndim_rate_code = \"hdfs://10.128.0.59:8020/data_warehouse/dim_rate_code\"\ndim_pickup_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_pickup_location\"\ndim_dropoff_location = \"hdfs://10.128.0.59:8020/data_warehouse/dim_dropoff_location\"\ndim_payment = \"hdfs://10.128.0.59:8020/data_warehouse/dim_payment\"\n\n# Result\npickup_kmean = \"uber-analysis-439804.query_result.kmean_pickup_location\"\ndropoff_kmean = \"uber-analysis-439804.query_result.kmean_dropoff_location\""}, {"cell_type": "code", "execution_count": 4, "id": "66bf784f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_pickup_location = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_pickup_location) \\\n    .load() \\\n    .withColumnRenamed(\"X\", \"pickup_x\") \\\n    .withColumnRenamed(\"Y\", \"pickup_y\")\n\ndf_dropoff_location = spark.read \\\n    .format(\"parquet\") \\\n    .option(\"path\", dim_dropoff_location) \\\n    .load() \\\n    .withColumnRenamed(\"X\", \"dropoff_x\") \\\n    .withColumnRenamed(\"Y\", \"dropoff_y\")\n\n# df_pickup_location.printSchema()\n# df_dropoff_location.printSchema()"}, {"cell_type": "code", "execution_count": 5, "id": "ce9abc4e", "metadata": {}, "outputs": [], "source": "# Normalizing data\nvector_assembler_pu = VectorAssembler(\n    inputCols=[\"pickup_x\", \"pickup_y\"], \n    outputCol=\"features\"\n)\n\nvector_assembler_do = VectorAssembler(\n    inputCols=[\"dropoff_x\", \"dropoff_y\"],\n    outputCol=\"features\"\n)\n\ndf_pickup_location = vector_assembler_pu.transform(df_pickup_location)\ndf_dropoff_location = vector_assembler_do.transform(df_dropoff_location)"}, {"cell_type": "code", "execution_count": 6, "id": "76b7a9bd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "scaler = StandardScaler(\n    inputCol=\"features\", \n    outputCol=\"scaledFeatures\", \n    withMean=True, \n    withStd=True)\n\nscaler_model_pu = scaler.fit(df_pickup_location)\nscaler_model_do = scaler.fit(df_dropoff_location)\n\ndf_pickup_location = scaler_model_pu.transform(df_pickup_location)\ndf_dropoff_location = scaler_model_do.transform(df_dropoff_location)"}, {"cell_type": "code", "execution_count": 7, "id": "7b506c9b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# K-Means CLustering\nkmeans = KMeans(\n    k=5, \n    seed=42, \n    featuresCol=\"scaledFeatures\", \n    predictionCol=\"cluster\"\n)\n\nmodel_pu = kmeans.fit(df_pickup_location)\nmodel_do = kmeans.fit(df_dropoff_location)\n\ndf_pickup_location = model_pu.transform(df_pickup_location)\ndf_dropoff_location = model_do.transform(df_dropoff_location)\n\ndf_pickup_location = df_pickup_location.select(\n    \"PULocationID\",\n    \"pickup_x\",\n    \"pickup_y\",\n    \"zone\",\n    \"borough\",\n    \"service_zone\",\n    \"cluster\"\n)\n\ndf_dropoff_location = df_dropoff_location.select(\n    \"DOLocationID\",\n    \"dropoff_x\",\n    \"dropoff_y\",\n    \"zone\",\n    \"borough\",\n    \"service_zone\",\n    \"cluster\"\n)"}, {"cell_type": "code", "execution_count": 8, "id": "6e8d8459", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Write to BigQuery\ndf_pickup_location.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", pickup_kmean) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()\n\ndf_dropoff_location.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", dropoff_kmean) \\\n    .option(\"temporaryGcsBucket\", \"uber-pyspark-jobs/temp\") \\\n    .mode(\"overwrite\") \\\n    .save()"}, {"cell_type": "code", "execution_count": 9, "id": "54d9591d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+------------+-----------+--------------------+---------+------------+-------+\n|PULocationID|    pickup_x|   pickup_y|                zone|  borough|service_zone|cluster|\n+------------+------------+-----------+--------------------+---------+------------+-------+\n|          65|-73.98557106|40.69537261|Downtown Brooklyn...| Brooklyn|   Boro Zone|      3|\n|         243|-73.93282432|40.85867029|Washington Height...|Manhattan|   Boro Zone|      1|\n|          77|-73.89571716|40.66770187|East New York/Pen...| Brooklyn|   Boro Zone|      0|\n|         188|-73.94520016|40.65756006|Prospect-Lefferts...| Brooklyn|   Boro Zone|      0|\n|         149|-73.94847421|40.60655799|             Madison| Brooklyn|   Boro Zone|      0|\n+------------+------------+-----------+--------------------+---------+------------+-------+\nonly showing top 5 rows\n\n+------------+------------+-----------+--------------------+---------+------------+-------+\n|DOLocationID|   dropoff_x|  dropoff_y|                zone|  borough|service_zone|cluster|\n+------------+------------+-----------+--------------------+---------+------------+-------+\n|          65|-73.98557106|40.69537261|Downtown Brooklyn...| Brooklyn|   Boro Zone|      1|\n|         243|-73.93282432|40.85867029|Washington Height...|Manhattan|   Boro Zone|      4|\n|          77|-73.89571716|40.66770187|East New York/Pen...| Brooklyn|   Boro Zone|      2|\n|         188|-73.94520016|40.65756006|Prospect-Lefferts...| Brooklyn|   Boro Zone|      2|\n|          94|-73.90059101|40.85826076|       Fordham South|    Bronx|   Boro Zone|      4|\n+------------+------------+-----------+--------------------+---------+------------+-------+\nonly showing top 5 rows\n\n"}], "source": "df_pickup_location.show(5)\ndf_dropoff_location.show(5)"}, {"cell_type": "code", "execution_count": 10, "id": "853d989a", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "0f09be4b", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}